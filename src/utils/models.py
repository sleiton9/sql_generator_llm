# Desc: This file contains the code to interact with the LLM models and send prompts to it.
import logging
from config.config_yaml_loader import load_config
import json
import re

from google import genai
from google.genai import types

import openai

logger = logging.getLogger(__name__)

# Load configuration settings from a YAML file.
config = load_config()
LLM_MODEL = config.get("llm_model", "gemini").lower().strip()
DEFAULT_CONTEXT = config.get("default_context")
TEMPERATURE = config.get("temperature", 0.5)
TOP_P = config.get("top_p", 0.5)
MAX_OUTPUT_TOKENS = config.get("max_output_tokens", 1024)
TOP_K = config.get("top_k", 50)

global_gemini_chat = None   # Global variable to store the initialized Gemini chat object.


def send_prompt(prompt: str) -> str:
    """
    Sends a prompt to the selected LLM model and returns its response.

    Currently supports both the Gemini and ChatGPT models, choosing the method
    based on the value of the global LLM_MODEL variable.

    Parameters:
        prompt (str): The message or prompt to send.

    Returns:
        str: The response generated by the LLM model.

    Raises:
        ValueError: If the configured model is not supported.
    """
    if LLM_MODEL == "gemini":
        return chat_gemini(prompt)
    elif LLM_MODEL == "chatgpt":
        return chat_chatgpt(prompt)
    else:
        raise ValueError(f"LLM model '{LLM_MODEL}' is not supported")



def initialize_gemini() -> genai.chats.Chat:
    """
    Initialize and return the Gemini chat object.

    This function creates a Gemini chat client if it has not been initialized yet.
    If it has already been initialized, the existing instance is returned.

    Returns:
        genai.chats.Chat: The initialized Gemini chat object.
    """
    global global_gemini_chat
    if global_gemini_chat is None:
        # Gemini config values
        GEMINI_API_KEY = config.get("gemini_api_key")
        GEMINI_MODEL = config.get("gemini_model", "gemini-2.0-flash")
        
        logger.info("Initialize Gemini")
        client_gemini = genai.Client(api_key=GEMINI_API_KEY)


        generation_config = types.GenerateContentConfig(
            system_instruction=DEFAULT_CONTEXT,
            temperature=TEMPERATURE,
            top_p=TOP_P,
            max_output_tokens=MAX_OUTPUT_TOKENS,
            top_k=TOP_K
        )

        global_gemini_chat = client_gemini.chats.create(
            model=GEMINI_MODEL,
            config=generation_config
        )
    return global_gemini_chat

def chat_gemini(prompt: str) -> str:
    """
    Send a prompt to the Gemini model and return the generated response text.

    This function uses the globally initialized Gemini chat object to send a message.

    Parameters:
        prompt (str): The prompt message to be sent to the Gemini model.

    Returns:
        str: The text response generated by the Gemini model.
    """
    logger.info("Sending prompt to Gemini model: " + prompt)
    chat = initialize_gemini()
    response = chat.send_message(prompt)
    logger.info("Response from Gemini: " + response.text)
    return response.text


def chat_chatgpt(prompt: str) -> str:
    """
    Sends a prompt to the ChatGPT model and returns the text of the generated response.

    Parameters:
        prompt (str): The message or prompt sent to the model.

    Returns:
        str: The response generated by ChatGPT.
    """
    logger.info("Sending prompt to ChatGPT model: " + prompt)
    CHATGPT_API_KEY = config.get("chatgpt_api_key")
    CHATGPT_MODEL = config.get("chatgpt_model", "gpt-3.5-turbo")
    openai.api_key = CHATGPT_API_KEY

    response = openai.chat.completions.create(
        model=CHATGPT_MODEL,
        messages=[
            {"role": "system", "content": DEFAULT_CONTEXT},
            {"role": "user", "content": prompt}
        ],
        temperature=TEMPERATURE,
        max_tokens=MAX_OUTPUT_TOKENS
    )

    answer = response.choices[0].message.content
    logger.info("Response from ChatGPT: " + answer)
    return answer


def extract_json_from_code_block(response_text: str) -> str | None:
    """
    Finds a code block delimited by triple backticks.
    Optionally matches "json" after the opening backticks (```json),
    and returns the raw content inside the code block.
    Returns None if not found.
    """
    # The pattern allows "json" (in any uppercase/lowercase combination)
    # to be optional: (?:json)?, thanks to the re.IGNORECASE flag.
    pattern = r"```(?:json)?\s*(.*?)\s*```"
    match = re.search(pattern, response_text, re.DOTALL | re.IGNORECASE)
    if match:
        return match.group(1).strip()
    return None

def get_sql_and_explanation(response_text: str) -> tuple[str | None, str | None]:
    """
    Tries to extract JSON from triple backticks, then parse it. Expects 'sql_statement' and 'explanation'.
    
    Args:
        response_text (str): The text response from the model.
    
    Returns:
        tuple[str | None, str | None]: A tuple containing the SQL statement and explanation strings
        extracted from the JSON, or None if not found.
    """
    json_content = extract_json_from_code_block(response_text)
    if not json_content:
        return None, None

    try:
        data = json.loads(json_content)
        sql_statement = data.get("sql_statement")
        explanation = data.get("explanation")
        return sql_statement, explanation
    except json.JSONDecodeError:
        return None, None
    
def generate_prompt(question: str, context: str, sql_motor: str, table_name: str) -> str:
    """
    Generate a prompt for the LLM model based on the user's question and context.

    Parameters:
        question (str): The user's question in natural language.
        context (str): The context of the database to base the query on.
        sql_motor (str): The SQL engine to be used.
        table_name (str): The name of the table to be queried.

    Returns:
        str: The generated prompt for the LLM model.
    """
    prompt = f"""
                    El usuario te hará una pregunta en lenguaje natural y tu salida debe ser
                    un objeto JSON con dos claves:
                        1) "sql_statement": que contenga la sentencia SQL optimizada necesaria (sin explicaciones internas).
                        2) "explanation": que describa en lenguaje natural cómo interpretar o usar el resultado, agregando las columnas de la fuente y las agregaciones usadas
                            de la consulta (sin mencionar directamente que se genera un SQL internamente para generar el resultado). 
                            Si la pregunta NO está relacionada con la tabla o los datos, entonces:
                                - "sql_statement": null
                                - "explanation": "La pregunta no está relacionada con los datos."

                    El motor SQL será: {sql_motor}
                    El nombre de la tabla es: {table_name}

                    Este es el contexto de la base de datos en la cual se debe basar la consulta:
                    {context}

                    La pregunta del usuario es: {question}

                    Por favor, devuelve la respuesta exclusivamente en un bloque de código delimitado por triple
                    backticks ``` con formato JSON válido, sin markdown.
                """ 
    return prompt

def generate_prompt_for_test(question: str, context: str, sql_motor: str) -> str:
    """
    Generate a prompt for the LLM model based on the test table question and context.

    Parameters:
        question (str): The question in natural language.
        context (str): The context of the database to base the query on.
        sql_motor (str): The SQL engine to be used.

    Returns:
        str: The generated prompt for the LLM model.
    """
    prompt = f"""
                    El usuario te hará una pregunta en lenguaje natural ingles y tu salida debe ser
                    un objeto JSON con una clave:
                        1) "sql_statement": que contenga la sentencia SQL optimizada necesaria (sin explicaciones internas).
                            Si la pregunta NO está relacionada con el contexto o los datos, entonces:
                                - "sql_statement": null

                    El motor SQL será: {sql_motor}

                    Este es el contexto de la base de datos en la cual se debe basar la consulta:
                    {context}

                    La pregunta del usuario es: {question}

                    Por favor, devuelve la respuesta exclusivamente en un bloque de código delimitado por triple
                    backticks ``` con formato JSON válido, sin markdown.
                """ 
    return prompt

# Desc: This file contains the code to interact with the LLM models and send prompts to it.
import logging
from config.config_yaml_loader import load_config
from utils.data import read_json
import json
import re
import numpy as np

from google import genai
from google.genai import types

import openai

logger = logging.getLogger(__name__)

# Load configuration settings from a YAML file.
config = load_config()
PROMPT_METHOD = config.get("prompt_method", "zero_shot").lower().strip()
LLM_MODEL = config.get("llm_model", "gemini").lower().strip()
DEFAULT_CONTEXT = config.get("default_context")
TEMPERATURE = config.get("temperature", 0.2)
TOP_P = config.get("top_p", 1)
MAX_OUTPUT_TOKENS = config.get("max_output_tokens", 512)

# CHAT_GPT CONFIGURATION
CHATGPT_API_KEY = config.get("chatgpt_api_key")
CHATGPT_MODEL = config.get("chatgpt_model", "gpt-3.5-turbo")
chatgpt_conversation_history = [] # List to store the conversation history for ChatGPT.

#GEMINI CONFIGURATION
GEMINI_API_KEY = config.get("gemini_api_key")
GEMINI_MODEL = config.get("gemini_model", "gemini-2.0-flash")
GEMINI_EMBEDDING_MODEL = config.get("gemini_embedding_model")

global_gemini_chat = None   # Global variable to store the initialized Gemini chat object.
global_gemini_client = None # Global variable to store the initialized Gemini client object.

PATH_RAG_DB_CONTEXT_JSON = config.get("paths", {}).get("rag_db_context")

def send_prompt(prompt: str) -> str:
    """
    Sends a prompt to the selected LLM model and returns its response.

    Currently supports both the Gemini and ChatGPT models, choosing the method
    based on the value of the global LLM_MODEL variable.

    Parameters:
        prompt (str): The message or prompt to send.

    Returns:
        str: The response generated by the LLM model.

    Raises:
        ValueError: If the configured model is not supported.
    """
    if LLM_MODEL == "gemini":
        return chat_gemini(prompt)
    elif LLM_MODEL == "chatgpt":
        return chat_chatgpt(prompt)
    else:
        raise ValueError(f"LLM model '{LLM_MODEL}' is not supported")


### GEMINI FUNTIONS ###
def initialize_gemini() -> genai.chats.Chat:
    """
    Initialize and return the Gemini chat object.

    This function creates a Gemini chat client if it has not been initialized yet.
    If it has already been initialized, the existing instance is returned.

    Returns:
        genai.chats.Chat: The initialized Gemini chat object.
    """
    global global_gemini_chat
    global global_gemini_client
    if global_gemini_chat is None:      
        logger.info("Initialize Gemini")
        global_gemini_client = genai.Client(api_key=GEMINI_API_KEY)


        generation_config = types.GenerateContentConfig(
            system_instruction=DEFAULT_CONTEXT,
            temperature=TEMPERATURE,
            top_p=TOP_P,
            max_output_tokens=MAX_OUTPUT_TOKENS
        )

        global_gemini_chat = global_gemini_client.chats.create(
            model=GEMINI_MODEL,
            config=generation_config
        )
    return global_gemini_chat, global_gemini_client

def chat_gemini(prompt: str) -> str:
    """
    Send a prompt to the Gemini model and return the generated response text.

    This function uses the globally initialized Gemini chat object to send a message.

    Parameters:
        prompt (str): The prompt message to be sent to the Gemini model.

    Returns:
        str: The text response generated by the Gemini model.
    """
    logger.info("Sending prompt to Gemini model: " + prompt)
    chat, _ = initialize_gemini()
    response = chat.send_message(prompt)
    logger.info("Response from Gemini: " + response.text)
    logger.info("Token count: " + str(response.usage_metadata))
    return response.text


### CHATGPT FUNCTIONS ###
def add_message_to_history(role: str, content: str) -> None:
    """
    Adds a message to the conversation history.
    
    Parameters:
        role (str): The role of the message ("user" or "assistant").
        content (str): The message content.
    """
    chatgpt_conversation_history.append({"role": role, "content": content})

def chat_chatgpt(prompt: str) -> str:
    """
    Sends a prompt to the ChatGPT model including the conversation context.
    
    This function updates the conversation history with the new user prompt,
    sends the complete context (starting with the default system context),
    receives the assistant's response, and adds it to the history.
    
    Parameters:
        prompt (str): The new prompt from the user.
    
    Returns:
        str: The response generated by ChatGPT.
    """
    logger.info("Sending prompt to ChatGPT model: " + prompt)
    openai.api_key = CHATGPT_API_KEY
    add_message_to_history("user", prompt)
    
    messages = [{"role": "system", "content": DEFAULT_CONTEXT}] + chatgpt_conversation_history

    response = openai.chat.completions.create(
        model=CHATGPT_MODEL,
        messages=messages,
        temperature=TEMPERATURE,
        max_tokens=MAX_OUTPUT_TOKENS
    )
    
    answer = response.choices[0].message.content
    add_message_to_history("assistant", answer)
    logger.info("Response from ChatGPT: " + answer)
    logger.info("Token count: " + str(response.usage))
    return answer


### PROMPT GENERATION FUNCTIONS ###
def extract_json_from_code_block(response_text: str) -> str | None:
    """
    Finds a code block delimited by triple backticks.
    Optionally matches "json" after the opening backticks (```json),
    and returns the raw content inside the code block.
    Returns None if not found.
    """
    # The pattern allows "json" (in any uppercase/lowercase combination)
    # to be optional: (?:json)?, thanks to the re.IGNORECASE flag.
    logger.info("Extracting JSON from code block in response text.")
    logger.debug("Response text: " + response_text)
    pattern = r"```(?:json)?\s*(.*?)\s*```"
    match = re.search(pattern, response_text, re.DOTALL | re.IGNORECASE)
    if match:
        return match.group(1).strip()
    logger.warning("No JSON code block found in the response text.")
    return None

def get_sql_and_explanation(response_text: str) -> tuple[str | None, str | None]:
    """
    Tries to extract JSON from triple backticks, then parse it. Expects 'sql_statement' and 'explanation'.
    
    Args:
        response_text (str): The text response from the model.
    
    Returns:
        tuple[str | None, str | None]: A tuple containing the SQL statement and explanation strings
        extracted from the JSON, or None if not found.
    """
    json_content = extract_json_from_code_block(response_text)
    if not json_content:
        return None, None

    try:
        data = json.loads(json_content)
        logger.info("Extracted JSON: " + str(data))
        sql_statement = data.get("sql_statement")
        explanation = data.get("explanation")
        logger.info("SQL Statement: " + str(sql_statement))
        logger.info("Explanation: " + str(explanation))
        return sql_statement, explanation
    except json.JSONDecodeError:
        return None, None
    
def generate_initial_prompt(context: str | None = None, sql_motor: str | None = None, table_name: str | None = None) -> str:
    """
    Generate a prompt for the LLM model based on method selected and on the context.

    For zero-shot method, the arg needed are the context, sql_motor and table_name.

    Parameters:
        context (str): The context of the database to base the query on.
        sql_motor (str): The SQL engine to be used.
        table_name (str): The name of the table to be queried.

    Returns:
        str: The generated prompt for the LLM model.
    """
    if PROMPT_METHOD == "zero_shot":
        prompt = config.get("prompt_zero_shot").format(
            instrucciones=config.get("prompt_general_instructions"),
            sql_motor=sql_motor, 
            table_name=table_name, 
            context=context)
    elif PROMPT_METHOD == "few_shot":
        prompt = config.get("prompt_few_shot").format(
            instrucciones=config.get("prompt_general_instructions"),
            sql_motor=sql_motor, 
            table_name=table_name, 
            context=context)
    elif PROMPT_METHOD == "rag":
        prompt = config.get("prompt_rag").format(
            instrucciones=config.get("prompt_general_instructions"))
    else:
        logger.error(f"Prompt method '{PROMPT_METHOD}' is not supported")
        raise ValueError(f"Prompt method '{PROMPT_METHOD}' is not supported")
    return prompt

def generate_question_prompt(question: str, context: str | None = None) -> str:
    """
    Generate a prompt for the LLM model based on the user's question.

    Parameters:
        question (str): The user's question in natural language.

    Returns:
        str: The generated prompt for the LLM model.
    """
    if PROMPT_METHOD == "rag":
        prompt = f"""### PREGUNTA DEL USUARIO ###
                    # {question}
                    
                    ### CONTEXTO ###
                    # {context}""" 
    else:
        prompt = f"""### PREGUNTA DEL USUARIO ###
                    # {question}"""
    return prompt


### RAG FUNCTIONS ###
def initialize_gemini_rag() -> list:
    """
    Initialize the Gemini model and generate embeddings for the database context.
    
    Args:
        None
    
    Returns:
        vector_store: A list representing the in-memory vector store containing embeddings.
    """
    initialize_gemini() 

    data = read_json(PATH_RAG_DB_CONTEXT_JSON)

    logger.info(f"Generating embeddings with model: {GEMINI_EMBEDDING_MODEL}...")
    embeddings_data = generate_gemini_embeddings(data, embedding_model_name=GEMINI_EMBEDDING_MODEL)

    logger.info(f"Generated {len(embeddings_data)} embeddings.")

    logger.info("Building in-memory vector store...")
    vector_store = build_vector_store(embeddings_data)
    logger.info(f"Vector store built with {len(vector_store)} valid entries.")
    return vector_store

def get_gemini_embedding(text: str, model_name, task_type) -> np.array:
    """
    Generates an embedding for the given text using the Gemini API.

    Args:
        text: The input text to generate the embedding for.
        model_name: The name of the Gemini embedding model to use.
        task_type: The task type for the embedding (e.g., RETRIEVAL_DOCUMENT, RETRIEVAL_QUERY).

    Returns:
        A numpy array representing the embedding.
        Returns None if an error occurs or the text is empty.
    """
    try:
        # Ensure the text is not empty or just whitespace
        if not text or not text.strip():
            logger.warning("Warning: Attempted to generate embedding for empty text.")
            return None
        

        response = global_gemini_client.models.embed_content(
            model=model_name,
            contents=text,
            config=types.EmbedContentConfig(task_type=task_type) 
        )
        # The embedding is directly under the 'embedding' key
        embedding_list = response.embeddings
        # Extract the 'values' attribute from each ContentEmbedding object
        embedding_values = [embedding.values for embedding in embedding_list]

        embedding_array = np.array(embedding_values)
        return embedding_array.flatten()
    except Exception as e:
        logger.error(f"Error generating embedding for text: '{text[:50]}...'")
        logger.error(f"Exception: {e}")
        return None # Return None in case of error

def generate_gemini_embeddings(data: dict, embedding_model_name) -> list:
    """
    Processes the JSON database schema, extracts information for each column,
    constructs a descriptive text, and generates an embedding using the Gemini API.

    Args:
        data: The JSON dictionary containing the database schema.
        embedding_model_name: The Gemini embedding model to use.

    Returns:
        A list of dictionaries, each containing:
            - 'text': The descriptive text of the column.
            - 'embedding': The numpy array for the text embedding.
            - 'metadata': Additional information such as database, table, and column name.
        Returns an empty list if the input data is invalid or no embeddings can be generated.
    """
    if not data or "Bases_de_datos" not in data:
        logger.error("Error: Invalid input data format for generating embeddings.")
        return []

    embeddings_data = []

    # Loop through each database
    for db in data.get("Bases_de_datos", []):
        db_name = db.get("Nombre_base_de_datos", "BaseDesconocida")
        db_type = db.get("Tipo_base_de_datos", "TipoDesconocido")

        # Loop through each table in the database
        for table in db.get("Tablas", []):
            table_name = table.get("Nombre_tabla", "TablaDesconocida") 

            # Process each column of the table
            for col in table.get("Columnas", []):
                col_name = col.get("Nombre_del_campo", "CampoDesconocido") 
                col_type = col.get("Tipo_campo", "TipoDesconocido")         
                col_def = col.get("Definicion", "DefinicionDesconocida") 
                col_ejem = col.get("Ejemplos", "EjemploDesconocido")      
                col_rel = col.get("Relaciones", "None")                
                col_rest = col.get("Restricciones", "None")            

                # Construct the descriptive text for the column 
                text = (
                    f"Bases_de_datos: {db_name}\n"
                    f"Tipo_Base_de_datos: {db_type}\n"
                    f"Tabla: {table_name}\n"
                    f"Columna: {col_name}\n"
                    f"Tipo: {col_type}\n"
                    f"Definicion: {col_def}\n"
                    f"Ejemplos: {col_ejem}\n"
                    f"Relaciones: {col_rel}\n"
                    f"Restricciones: {col_rest}"
                )

                # Generate the embedding using the Gemini model
                # Use RETRIEVAL_DOCUMENT because these texts will be stored for retrieval
                embedding = get_gemini_embedding(text,
                                                 model_name=embedding_model_name,
                                                 task_type="RETRIEVAL_DOCUMENT")

                # Only add if the embedding was generated successfully
                if embedding is not None:
                    embeddings_data.append({
                        "text": text,
                        "embedding": embedding,
                        "metadata": {
                            "Bases_de_datos": db_name,
                            "Tabla": table_name,
                            "Columna": col_name,
                            "Definicion": col_def,
                            "Ejemplos": col_ejem,
                        }
                    })
                else:
                    # Log if embedding failed for a specific column
                    logger.error(f"Could not generate embedding for: {db_name}.{table_name}.{col_name}")

    return embeddings_data

def build_vector_store(embeddings_data: list) -> list:
    """
    Constructs a simple in-memory vector store from the embeddings data.
    (In a real application, you would use a vector database like ChromaDB, Pinecone, etc.)

    Args:
        embeddings_data: A list of dictionaries that include embedding, text, and metadata.

    Returns:
        A list representing the in-memory vector store.
    """
    vector_store = []
    for item in embeddings_data:
        # Check if the embedding exists and is a valid numpy array before adding
        if "embedding" in item and isinstance(item["embedding"], np.ndarray):
             vector_store.append({
                "embedding": item["embedding"],
                "text": item.get("text", ""),
                "metadata": item.get("metadata", {})
            })
        else:
            # Log if an item is skipped due to invalid embedding
            logger.warning(f"Warning: Skipped an item without a valid embedding when building the vector store. Metadata: {item.get('metadata', {})}")
    return vector_store

def get_relevant_context_gemini(query: str,
                                vector_store: list,
                                threshold: float = 0.7) -> list:
    """
    Generates an embedding for the query using Gemini, calculates the cosine similarity
    with each embedding in the vector_store, and returns those results whose similarity
    is greater than or equal to the specified threshold.
    
    Args:
        query: The natural language query.
        vector_store: The in-memory vector store containing precomputed embeddings.
        threshold: The minimum cosine similarity threshold that a result must meet to be included.
    
    Returns:
        A list of dictionaries, each containing:
            - 'score': The cosine similarity value.
            - 'text': The associated descriptive text in the vector_store.
            - 'metadata': Associated metadata (e.g., database, table, column name).
        Returns an empty list if the embedding cannot be generated or if the vector_store is empty.
    """
    if not vector_store:
        logger.warning("Warning: The vector store is empty.")
        return []

    # Generar la embedding para la consulta usando Gemini.
    query_embedding = get_gemini_embedding(query,
                                           model_name=GEMINI_EMBEDDING_MODEL,
                                           task_type="RETRIEVAL_QUERY")
    if query_embedding is None:
        logger.error("Error: Could not generate the embedding for the query.")
        return []
    
    similarities = []
    for entry in vector_store:
        emb = entry["embedding"]
        # Calcular la similitud coseno
        dot = np.dot(query_embedding, emb)
        norm_q = np.linalg.norm(query_embedding)
        norm_e = np.linalg.norm(emb)
        
        # Evitar división por cero si algún vector tiene norma 0
        if norm_q == 0 or norm_e == 0:
            cosine_sim = 0.0
        else:
            cosine_sim = dot / (norm_q * norm_e)
        
        similarities.append((cosine_sim, entry))

    # Ordenar los resultados de mayor a menor similitud
    similarities.sort(key=lambda x: x[0], reverse=True)
    
    # Filtrar los resultados que cumplan con el umbral (threshold)
    filtered_results = [ (sim, entry) for sim, entry in similarities if sim >= threshold ]

    # Formatear y retornar el output
    return [
        {
            "score": sim,
            "text": entry["text"],
            "metadata": entry["metadata"]
        }
        for sim, entry in filtered_results
    ]




### TESTING FUNCTIONS HUGGING FACE ###
def send_unique_prompt_gemini(prompt: str) -> str:
    """
    Send a test prompt to the Gemini model and return the generated response text.

    Without historical context, this function is used for testing purposes.

    Parameters:
        prompt (str): The prompt message to be sent to the Gemini model.

    Returns:
        str: The text response generated by the Gemini model.
    """
    logger.info("Sending prompt without history to Gemini model: " + prompt)
    GEMINI_API_KEY = config.get("gemini_api_key")
    GEMINI_MODEL = config.get("gemini_model", "gemini-2.0-flash")

    generation_config = types.GenerateContentConfig(
            system_instruction=DEFAULT_CONTEXT,
            temperature=TEMPERATURE,
            top_p=TOP_P,
            max_output_tokens=MAX_OUTPUT_TOKENS
        )

    client = genai.Client(api_key=GEMINI_API_KEY)
    response = client.models.generate_content(
    model=GEMINI_MODEL, contents=prompt, config=generation_config
    )
    logger.info("Response from Gemini: " + response.text)
    logger.info("Token count: " + str(response.usage_metadata))
    logger.info("Temperature: " + str(TEMPERATURE))
    logger.info("Top P: " + str(TOP_P))
    return response.text

def send_unique_prompt_chatgpt(prompt: str) -> str:
    """
    Sends a prompt to the ChatGPT model and returns the text of the generated response.

    Without historical context, this function is used for testing purposes.

    Parameters:
        prompt (str): The message or prompt sent to the model.

    Returns:
        str: The response generated by ChatGPT.
    """
    logger.info("Sending prompt without history to ChatGPT model: " + prompt)
    openai.api_key = CHATGPT_API_KEY

    response = openai.chat.completions.create(
        model=CHATGPT_MODEL,
        messages=[
            {"role": "system", "content": DEFAULT_CONTEXT},
            {"role": "user", "content": prompt}
        ],
        temperature=TEMPERATURE,
        max_tokens=MAX_OUTPUT_TOKENS
    )

    answer = response.choices[0].message.content
    logger.info("Response from ChatGPT: " + answer)
    logger.info("Token count: " + str(response.usage))
    return answer

def generate_prompt_for_test_hugging_face(question: str, context: str, sql_motor: str, tipo: str) -> str:
    """
    Generate a prompt for the LLM model based on the test table question and context.

    Parameters:
        question (str): The question in natural language.
        context (str): The context of the database to base the query on.
        sql_motor (str): The SQL engine to be used.

    Returns:
        str: The generated prompt for the LLM model.
    """
    if tipo == "malo" and PROMPT_METHOD == "zero_shot":
        prompt = f"""El usuario te hará una pregunta en lenguaje natural en ingles y tu salida debe ser EXCLUSIVAMENTE un objeto JSON con una clave: 1) "sql_statement": que contenga la sentencia SQL optimizada necesaria (sin explicaciones internas). El motor SQL será: {sql_motor}. Este es el esquema de las tablas en las que se debe basar la consulta: {context}. La pregunta del usuario: {question}. Por favor, razona paso a paso cómo traducir la pregunta a SQL. Sin embargo, devuelve solo EXCLUSIVAMENTE el JSON en un bloque de código delimitado por triple backticks ``` con la llave indicada.""" 
    elif tipo == "malo" and PROMPT_METHOD == "few_shot":
        prompt = f"""El usuario te hará una pregunta en lenguaje natural en ingles y tu salida debe ser EXCLUSIVAMENTE un objeto JSON con una clave: 1) "sql_statement": que contenga la sentencia SQL optimizada necesaria (sin explicaciones internas). El motor SQL será: {sql_motor}. Este es el esquema de las tablas en las que se debe basar la consulta: {context}. La pregunta del usuario: {question}. Algunos ejemplos son: 1. Para esta pregunta What are the names of the states where at least 3 heads were born? El resultado esperado es: SELECT born_state FROM head GROUP BY born_state HAVING COUNT(*) >= 3. 2. Para esta pregunta What are the distinct creation years of the departments managed by a secretary born in state 'Alabama'? El resultado esperado es: SELECT DISTINCT T1.creation FROM department AS T1 JOIN management AS T2 ON T1.department_id = T2.department_id JOIN head AS T3 ON T2.head_id = T3.head_id WHERE T3.born_state = 'Alabama'. 3. Para esta pregunta What is the average latitude and longitude of stations located in San Jose city? El resultado esperado es: SELECT AVG(lat), AVG(long) FROM station WHERE city = "San Jose". Por favor, razona paso a paso cómo traducir la pregunta a SQL. Sin embargo, devuelve solo EXCLUSIVAMENTE el JSON en un bloque de código delimitado por triple backticks ``` con la llave indicada."""
    elif tipo == "bueno" and PROMPT_METHOD == "zero_shot":
        prompt = f"""
                    ### INSTRUCCIONES ###
                    # El usuario te hará una pregunta en lenguaje natural en ingles y tu salida debe ser EXCLUSIVAMENTE un objeto JSON con una clave:
                    #    1) "sql_statement": que contenga la sentencia SQL optimizada necesaria (sin explicaciones internas).

                    ### CONTEXTO ###
                    # El motor SQL será: {sql_motor}
                    # Este es el esquema de las tablas en las que se debe basar la consulta:
                    # {context}

                    ### PREGUNTA DEL USUARIO ###
                    # {question}

                    ## RESPUESTA ###
                    # Para el llamado de alias de tablas usar T1, T2, T3, etc, solo cuando sea necesario.
                    # Usar el operador "<>" para comparar valores de columnas y no usar el operador "!=".
                    # No agregar alias a las columnas si se aplica una función de agregación, como COUNT, SUM, etc.
                    # Las funciones de agregacion deben ser en mayusculas.
                    # Por favor, razona paso a paso cómo traducir la pregunta a SQL. Sin embargo, devuelve solo EXCLUSIVAMENTE el JSON en un bloque de código delimitado por triple backticks ``` con la llave indicada.
                """ 
    elif tipo == "bueno" and PROMPT_METHOD == "few_shot":
        prompt = f"""
                    ### INSTRUCCIONES ###
                    # El usuario te hará una pregunta en lenguaje natural en ingles y tu salida debe ser EXCLUSIVAMENTE un objeto JSON con una clave:
                    #    1) "sql_statement": que contenga la sentencia SQL optimizada necesaria (sin explicaciones internas).

                    ### CONTEXTO ###
                    # El motor SQL será: {sql_motor}
                    # Este es el esquema de las tablas en las que se debe basar la consulta:
                    # {context}

                    ### PREGUNTA DEL USUARIO ###
                    # {question}

                    ## EJEMPLOS ###
                    # Para esta pregunta What are the names of the states where at least 3 heads were born? El resultado esperado es: SELECT born_state FROM head GROUP BY born_state HAVING COUNT(*) >= 3
                    # Para esta pregunta What are the distinct creation years of the departments managed by a secretary born in state 'Alabama'? El resultado esperado es: SELECT DISTINCT T1.creation FROM department AS T1 JOIN management AS T2 ON T1.department_id = T2.department_id JOIN head AS T3 ON T2.head_id = T3.head_id WHERE T3.born_state = 'Alabama'
                    # Para esta pregunta What is the average latitude and longitude of stations located in San Jose city? El resultado esperado es: SELECT AVG(lat), AVG(long) FROM station WHERE city = "San Jose"

                    ## RESPUESTA ###
                    # Para el llamado de alias de tablas usar T1, T2, T3, etc, solo cuando sea necesario.
                    # Usar el operador "<>" para comparar valores de columnas y no usar el operador "!=".
                    # No agregar alias a las columnas si se aplica una función de agregación, como COUNT, SUM, etc.
                    # Las funciones de agregacion deben ser en mayusculas.
                    # Por favor, razona paso a paso cómo traducir la pregunta a SQL. Sin embargo, devuelve solo EXCLUSIVAMENTE el JSON en un bloque de código delimitado por triple backticks ``` con la llave indicada.
                """ 
    return prompt

def generate_prompt_for_test_nutresa(question: str, context: str, sql_motor: str, tipo: str, table_name: str) -> str:
    """
    Generate a prompt for the LLM model based on the test table question and context.

    Parameters:
        question (str): The question in natural language.
        context (str): The context of the database to base the query on.
        sql_motor (str): The SQL engine to be used.

    Returns:
        str: The generated prompt for the LLM model.
    """
    if tipo == "malo" and (PROMPT_METHOD == "zero_shot" or PROMPT_METHOD == "rag"):
        prompt = f"""El usuario te hará una pregunta en lenguaje natural y tu salida debe ser EXCLUSIVAMENTE un objeto JSON con una clave: 1) "sql_statement": que contenga la sentencia SQL optimizada necesaria (sin explicaciones internas). El motor SQL será: {sql_motor}. El nombre de la tabla es: {table_name}. Este es el contexto de la tabla en la cual se debe basar la consulta: {context}. La pregunta del usuario es {question}. Por favor, razona paso a paso cómo traducir la pregunta a SQL. Sin embargo, devuelve EXCLUSIVAMENTE el JSON en un bloque de código delimitado por triple backticks ``` con las dos llaves indicadas. Generar un solo SQL a la vez, no generar varios SQLs. Si se requiere generar varios SQLs dejar null la llave de sql_statement y generar la explicacion de que no se puede generar varias consultas al tiempo. No usar alias en campos con agregación.""" 
    elif tipo == "malo" and PROMPT_METHOD == "few_shot":
        prompt = f"""El usuario te hará una pregunta en lenguaje natural y tu salida debe ser EXCLUSIVAMENTE un objeto JSON con una clave: 1) "sql_statement": que contenga la sentencia SQL optimizada necesaria (sin explicaciones internas). El motor SQL será: {sql_motor}. El nombre de la tabla es: {table_name}. Este es el contexto de la tabla en la cual se debe basar la consulta: {context}. La pregunta del usuario es {question}. Ejemplos: Para esta pregunta ¿Cuál fue la cantidad total vendida por sector del producto en 2024 en la ciudad Bogota? El resultado esperado es: SELECT DES_SECTOR, SUM(VENTA_VOLUMEN) FROM df WHERE ANIO= 2024 AND DES_CIUDAD= 'Bogota' GROUP BY DES_SECTOR; Para esta pregunta ¿Cuántas ventas se realizaron en el año 2023? El resultado esperado es: SELECT COUNT(*) FROM df WHERE ANIO= 2023; Para esta pregunta ¿Cuál es el precio promedio de venta por línea y tipo de fabricante en el ultimo año? El resultado esperado es: SELECT DES_LINEA, DES_TIPO_FABRICANTE, AVG(VENTA_PRECIO) FROM df WHERE ANIO= 2024 GROUP BY DES_LINEA, DES_TIPO_FABRICANTE;. Por favor, razona paso a paso cómo traducir la pregunta a SQL. Sin embargo, devuelve EXCLUSIVAMENTE el JSON en un bloque de código delimitado por triple backticks ``` con las dos llaves indicadas. Generar un solo SQL a la vez, no generar varios SQLs. Si se requiere generar varios SQLs dejar null la llave de sql_statement y generar la explicacion de que no se puede generar varias consultas al tiempo. No usar alias en campos con agregación."""
    elif tipo == "bueno" and (PROMPT_METHOD == "zero_shot" or PROMPT_METHOD == "rag"):
        prompt = f"""
                    ### INSTRUCCIONES ###
                    # El usuario te hará una pregunta en lenguaje natural y tu salida debe ser EXCLUSIVAMENTE un objeto JSON con una clave:
                    #    1) "sql_statement": que contenga la sentencia SQL optimizada necesaria (sin explicaciones internas).

                    ### CONTEXTO ###
                    # El motor SQL será: {sql_motor}
                    # El nombre de la tabla es: {table_name}
                    # Este es el contexto de la tabla en la cual se debe basar la consulta: 
                    # {context}

                    ### PREGUNTA DEL USUARIO ###
                    # {question}

                    ## RESPUESTA ###
                    # No usar alias en campos con agregación.
                    # Por favor, razona paso a paso cómo traducir la pregunta a SQL. Sin embargo, devuelve EXCLUSIVAMENTE el JSON en un bloque de código delimitado por triple backticks ``` con las dos llaves indicadas.
                    # Generar un solo SQL a la vez, no generar varios SQLs. Si se requiere generar varios SQLs dejar null la llave de sql_statement y generar la explicacion de que no se puede generar varias consultas al tiempo.
                """ 
    elif tipo == "bueno" and PROMPT_METHOD == "few_shot":
        prompt = f"""
                    ### INSTRUCCIONES ###
                    # El usuario te hará una pregunta en lenguaje natural y tu salida debe ser EXCLUSIVAMENTE un objeto JSON con una clave:
                    #    1) "sql_statement": que contenga la sentencia SQL optimizada necesaria (sin explicaciones internas).

                    ### CONTEXTO ###
                    # El motor SQL será: {sql_motor}
                    # El nombre de la tabla es: {table_name}
                    # Este es el contexto de la tabla en la cual se debe basar la consulta: 
                    # {context}

                    ### PREGUNTA DEL USUARIO ###
                    # {question}

                    ## EJEMPLOS ###
                    # Para esta pregunta ¿Cuál fue la cantidad total vendida por sector del producto en 2024 en la ciudad Bogota? El resultado esperado es: SELECT DES_SECTOR, SUM(VENTA_VOLUMEN) FROM df WHERE ANIO= 2024 AND DES_CIUDAD= 'Bogota' GROUP BY DES_SECTOR;
                    # Para esta pregunta ¿Cuántas ventas se realizaron en el año 2023? El resultado esperado es: SELECT COUNT(*) FROM df WHERE ANIO= 2023;
                    # Para esta pregunta ¿Cuál es el precio promedio de venta por línea y tipo de fabricante en el ultimo año? El resultado esperado es: SELECT DES_LINEA, DES_TIPO_FABRICANTE, AVG(VENTA_PRECIO) FROM df WHERE ANIO= 2024 GROUP BY DES_LINEA, DES_TIPO_FABRICANTE;
                    
                    ## RESPUESTA ###
                    # No usar alias en campos con agregación.
                    # Por favor, razona paso a paso cómo traducir la pregunta a SQL. Sin embargo, devuelve EXCLUSIVAMENTE el JSON en un bloque de código delimitado por triple backticks ``` con las dos llaves indicadas.
                    # Generar un solo SQL a la vez, no generar varios SQLs. Si se requiere generar varios SQLs dejar null la llave de sql_statement y generar la explicacion de que no se puede generar varias consultas al tiempo.
                """ 
    return prompt

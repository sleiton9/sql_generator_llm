import logging
from typing import Any
from config.config_yaml_loader import load_config

from google import genai
from google.genai import types

logger = logging.getLogger(__name__)

# Load configuration settings from a YAML file.
config = load_config()
LLM_MODEL = config.get("llm_model", "gemini").lower().strip()

# Gemini config values
GEMINI_API_KEY = config.get("gemini_api_key")
GEMINI_MODEL = config.get("gemini_model")
GEMINI_DEFAULT_CONTEXT = config.get("gemini_default_context")
# Global variable to store the initialized Gemini chat object.
global_gemini_chat = None


def initialize_gemini() -> Any:
    """
    Initialize and return the Gemini chat object.

    This function creates a Gemini chat client if it has not been initialized yet.
    If it has already been initialized, the existing instance is returned.

    Returns:
        Any: The initialized Gemini chat object.
    """
    global global_gemini_chat
    if global_gemini_chat is None:
        logger.info("Initialize Gemini")
        client_gemini = genai.Client(api_key=GEMINI_API_KEY)
        global_gemini_chat = client_gemini.chats.create(
            model=GEMINI_MODEL,
            config=types.GenerateContentConfig(system_instruction=GEMINI_DEFAULT_CONTEXT)
        )
    return global_gemini_chat

def chat_gemini(prompt: str) -> str:
    """
    Send a prompt to the Gemini model and return the generated response text.

    This function uses the globally initialized Gemini chat object to send a message.

    Parameters:
        prompt (str): The prompt message to be sent to the Gemini model.

    Returns:
        str: The text response generated by the Gemini model.
    """
    logger.info("Sending prompt to Gemini model: " + prompt)
    chat = initialize_gemini()
    response = chat.send_message(prompt)
    logger.info("Response from Gemini: " + response.text)
    return response.text

def send_prompt(prompt: str) -> str:
    """
    Send a prompt to the selected LLM model and return its response.

    Currently, only the Gemini model is supported.

    Parameters:
        prompt (str): The prompt message to send to the LLM model.

    Returns:
        str: The text response generated by the LLM model.

    Raises:
        ValueError: If the configured LLM model is not supported.
    """
    if LLM_MODEL == "gemini":
        return chat_gemini(prompt)
    else:
        raise ValueError(f"LLM model '{LLM_MODEL}' is not supported")

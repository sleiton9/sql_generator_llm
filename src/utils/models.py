# Desc: This file contains the code to interact with the LLM models and send prompts to it.
import logging
from config.config_yaml_loader import load_config
import json
import re

from google import genai
from google.genai import types

logger = logging.getLogger(__name__)

# Load configuration settings from a YAML file.
config = load_config()
LLM_MODEL = config.get("llm_model", "gemini").lower().strip()

# Gemini config values
GEMINI_API_KEY = config.get("gemini_api_key")
GEMINI_MODEL = config.get("gemini_model")
GEMINI_DEFAULT_CONTEXT = config.get("gemini_default_context")
global_gemini_chat = None   # Global variable to store the initialized Gemini chat object.


def initialize_gemini() -> genai.chats.Chat:
    """
    Initialize and return the Gemini chat object.

    This function creates a Gemini chat client if it has not been initialized yet.
    If it has already been initialized, the existing instance is returned.

    Returns:
        genai.chats.Chat: The initialized Gemini chat object.
    """
    global global_gemini_chat
    if global_gemini_chat is None:
        logger.info("Initialize Gemini")
        client_gemini = genai.Client(api_key=GEMINI_API_KEY)
        global_gemini_chat = client_gemini.chats.create(
            model=GEMINI_MODEL,
            config=types.GenerateContentConfig(system_instruction=GEMINI_DEFAULT_CONTEXT)
        )
    return global_gemini_chat

def chat_gemini(prompt: str) -> str:
    """
    Send a prompt to the Gemini model and return the generated response text.

    This function uses the globally initialized Gemini chat object to send a message.

    Parameters:
        prompt (str): The prompt message to be sent to the Gemini model.

    Returns:
        str: The text response generated by the Gemini model.
    """
    logger.info("Sending prompt to Gemini model: " + prompt)
    chat = initialize_gemini()
    response = chat.send_message(prompt)
    logger.info("Response from Gemini: " + response.text)
    return response.text

def send_prompt(prompt: str) -> str:
    """
    Send a prompt to the selected LLM model and return its response.

    Currently, only the Gemini model is supported.

    Parameters:
        prompt (str): The prompt message to send to the LLM model.

    Returns:
        str: The text response generated by the LLM model.

    Raises:
        ValueError: If the configured LLM model is not supported.
    """
    if LLM_MODEL == "gemini":
        return chat_gemini(prompt)
    elif LLM_MODEL == "chatgpt":
        pass #TODO Add connection and chat for chat GPT
    else:
        raise ValueError(f"LLM model '{LLM_MODEL}' is not supported")

def extract_json_from_code_block(response_text: str) -> str | None:
    """
    Finds a ```json ... ``` code block in the response_text and returns the raw JSON inside it.
    Returns None if not found.
    """
    pattern = r"```json\s*(.*?)\s*```"  # Captura el contenido entre ```json y ```
    match = re.search(pattern, response_text, re.DOTALL | re.IGNORECASE)
    if match:
        return match.group(1).strip()
    return None

def get_sql_and_explanation(response_text: str) -> tuple[str | None, str | None]:
    """
    Tries to extract JSON from triple backticks, then parse it. Expects 'sql_statement' and 'explanation'.
    
    Args:
        response_text (str): The text response from the model.
    
    Returns:
        tuple[str | None, str | None]: A tuple containing the SQL statement and explanation strings
        extracted from the JSON, or None if not found.
    """
    json_content = extract_json_from_code_block(response_text)
    if not json_content:
        return None, None

    try:
        data = json.loads(json_content)
        sql_statement = data.get("sql_statement")
        explanation = data.get("explanation")
        return sql_statement, explanation
    except json.JSONDecodeError:
        return None, None
